# -*- coding: utf-8 -*-
"""DLA4_Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gWp5V9EiOsFk3j0TwwEINObzHd_meO6T
"""

!nvidia-smi

"""# Mouting Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import Libraries"""

!pip install nltk==3.5
from nltk.translate.meteor_score import meteor_score
from nltk.translate.bleu_score import sentence_bleu
import random

from sklearn.model_selection import train_test_split
import datetime
import time
from PIL import Image
import collections
import random
from keras.models import load_model

import os
import cv2
import matplotlib.pyplot as plt
import joblib
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from keras.applications.vgg16 import VGG16
from keras.applications.vgg19 import VGG19
from keras.models import Model
import numpy as np
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('words')
from nltk.corpus import words
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

"""# Preprocessing"""

def creating_Data(path):
  images = []
  image_names = []
  count = 0 
  for root, dirs, files in os.walk(path):
    for file in files:
      count += 1
      image_names.append(file)
      img = plt.imread(os.path.join(root, file))
      img = cv2.resize(img, dsize=(224, 224))
      images.append(img)
      if count%500 == 0:
        print(count)
  return images, image_names

images, image_names = creating_Data('/content/drive/MyDrive/DL/Assignment_4/Data/Train/Images')
print(len(image_names))

# joblib.dump(images, '/content/drive/MyDrive/DL/Assignment_4/TrainImages')
# joblib.dump(image_names, '/content/drive/MyDrive/DL/Assignment_4/TrainImagesNames')

images = joblib.load('/content/drive/MyDrive/DL/Assignment_4/TrainImages')
image_names = joblib.load('/content/drive/MyDrive/DL/Assignment_4/TrainImagesNames')

images_val, image_names_val = creating_Data('/content/drive/MyDrive/DL/Assignment_4/Data/Val/Images')
print(len(image_names_val))

# joblib.dump(images_val, '/content/drive/MyDrive/DL/Assignment_4/ValImages')
# joblib.dump(image_names_val, '/content/drive/MyDrive/DL/Assignment_4/ValImagesNames')

images_val = joblib.load('/content/drive/MyDrive/DL/Assignment_4/ValImages')
image_names_val = joblib.load('/content/drive/MyDrive/DL/Assignment_4/ValImagesNames')

images_test, image_names_test = creating_Data('/content/drive/MyDrive/DL/Assignment_4/Data/Test/Images')
print(len(image_names_test))

plt.imshow(images_test[0])

# joblib.dump(images_test, '/content/drive/MyDrive/DL/Assignment_4/TestImages')
# joblib.dump(image_names_test, '/content/drive/MyDrive/DL/Assignment_4/TestImagesNames')

images_test = joblib.load('/content/drive/MyDrive/DL/Assignment_4/TestImages')
image_names_test = joblib.load('/content/drive/MyDrive/DL/Assignment_4/TestImagesNames')

images_all = images + images_val + images_test
print(len(images_all))

image_names_all = image_names + image_names_val + image_names_test

image_dataset_all = {}
for i in range(len(image_names_all)):
  image_dataset_all[image_names_all[i]] = images_all[i]

plt.imshow(image_dataset_all['1000268201_693b08cb0e.jpg'])

def Clean_Tagging(data):
  for j in sorted(data.keys()):
    for i in range(len(data[j])):
      data[j][i] = '<start> ' + data[j][i].lower() + ' <end>'

def convert_to_DataFrame(data):
  img_caption = []
  for i in sorted(data.keys()):
    for caption in data[i]:
      img_caption.append((i, caption))
  df = pd.DataFrame(img_caption, columns=['ImageName', 'Caption'])
  return df

data = pd.read_pickle('/content/drive/MyDrive/DL/Assignment_4/Data/Train/train_captions.pkl')
data_val = pd.read_pickle('/content/drive/MyDrive/DL/Assignment_4/Data/Val/val_captions.pkl')
data_test = pd.read_pickle('/content/drive/MyDrive/DL/Assignment_4/Data/Test/test_captions.pkl')

Clean_Tagging(data)
Clean_Tagging(data_val)
Clean_Tagging(data_test)

data.update(data_val)

data.update(data_test)

len(data)

data['103106960_e8a41d64f8.jpg']

df = convert_to_DataFrame(data)
# df_val = convert_to_DataFrame(data_val)
# df_test = convert_to_DataFrame(data_test)

df_train_val = df_train.append(df_val)

df_all = df_train_val.append(df_test)

df_all.tail(20)

corpus = np.array(df['Caption'])
print(len(corpus))

corpus[0]

corpus_val = np.array(df_val['Caption'])

print(len(corpus))

t=Tokenizer()
t.fit_on_texts(corpus)
text_matrix = t.texts_to_sequences(corpus)

t.word_index['start']

text_matrix[:5]

max_len = 0
for i in text_matrix:
  if max_len < len(i):
    max_len = len(i)
print(max_len)

text_pad = pad_sequences(text_matrix, maxlen=max_len+1, padding='post')
# text_pad_val = pad_sequences(text_matrix_val, maxlen=max_len+1, padding='post')
print(len(text_pad))
# print(text_pad[:5])

plt.imshow(image_dataset_all['103106960_e8a41d64f8.jpg'])

data_embeddings['997722733_0cb5439472.jpg']

text_pad[-5:]

data_embeddings = {}
count = 0
for img_nm in df['ImageName']:
  try:
    data_embeddings[img_nm].append(text_pad[count])
    count += 1
  except:
     data_embeddings[img_nm] = [text_pad[count]]
     count += 1

len(data_embeddings), len(image_dataset_all)

img_name_vec = np.array(df['ImageName'])
print(len(img_name_vec))

image_try = {}
count = 0
for i in df['ImageName']:
  image_try = image_dataset_all[i]

img_name_train, img_name_test, text_pad_train, text_pad_test = train_test_split(img_name_vec, text_pad, test_size=0.2, random_state=42)

img_name_train = np.array(df_train['ImageName'])
img_name_test = np.array(df_val['ImageName'])

print(img_name_train.shape, img_name_test.shape, text_pad_train.shape, text_pad_test.shape)

len(t.word_index)

# joblib.dump(image_dataset, '/content/drive/MyDrive/DL/Assignment_4/Dict_Images')
# joblib.dump(data_embeddings, '/content/drive/MyDrive/DL/Assignment_4/Dict_Embeddings')
# joblib.dump(captions, '/content/drive/MyDrive/DL/Assignment_4/Dict_Captions')

"""# Data Loading"""

captions = joblib.load('/content/drive/MyDrive/DL/Assignment_4/Dict_Captions')

captions['/content/drive/MyDrive/DL/Assignment_4/Data/Test/Images/1056338697_4f7d7ce270.jpg']

image_names = list(captions.keys())

train_captions = []
img_name_vector = []

for image_path in image_names[:6000]:
  caption_list = captions[image_path]
  train_captions.extend(caption_list)
  img_name_vector.extend([image_path] * len(caption_list))

def load_image(image_path):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (224, 224))
    img = tf.keras.applications.vgg16.preprocess_input(img)
    return img, image_path

image_model = tf.keras.applications.VGG16(include_top=False,
                                                weights='imagenet')
new_input = image_model.input
hidden_layer = image_model.layers[-1].output

image_features_extract_model = tf.keras.Model(new_input, hidden_layer)
image_features_extract_model.summary()

encode_train = sorted(set(img_name_vector))

image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)
image_dataset = image_dataset.map(
  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(32)

for img, path in image_dataset:
  batch_features = image_features_extract_model(img)
  batch_features = tf.reshape(batch_features,
                              (batch_features.shape[0], -1, batch_features.shape[3]))

  for bf, p in zip(batch_features, path):
    path_of_feature = p.numpy().decode("utf-8")
    np.save(path_of_feature, bf.numpy())

def calc_max_length(tensor):
    return max(len(t) for t in tensor)

top_k = 5000
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token="<unk>", filters='!"#$%&()*+.,-/:;=?@[\]^_`{|}~ ')
tokenizer.fit_on_texts(train_captions)

tokenizer.word_index['<pad>'] = 0
tokenizer.index_word[0] = '<pad>'

train_seqs = tokenizer.texts_to_sequences(train_captions)

cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')

max_length = calc_max_length(train_seqs)

img_to_cap_vector = collections.defaultdict(list)
for img, cap in zip(img_name_vector, cap_vector):
  img_to_cap_vector[img].append(cap)


img_keys = list(img_to_cap_vector.keys())
random.shuffle(img_keys)

slice_index = int(len(img_keys)*0.8)
img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]

img_name_train = []
cap_train = []
for imgt in img_name_train_keys:
  capt_len = len(img_to_cap_vector[imgt])
  img_name_train.extend([imgt] * capt_len)
  cap_train.extend(img_to_cap_vector[imgt])

img_name_val = []
cap_val = []
for imgv in img_name_val_keys:
  capv_len = len(img_to_cap_vector[imgv])
  img_name_val.extend([imgv] * capv_len)
  cap_val.extend(img_to_cap_vector[imgv])

len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)

def map_func(img_name, cap):
  img_tensor = np.load(img_name.decode('utf-8')+'.npy')
  return img_tensor, cap

dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))

dataset = dataset.map(lambda item1, item2: tf.numpy_function(
          map_func, [item1, item2], [tf.float32, tf.int32]),
          num_parallel_calls=tf.data.AUTOTUNE)

dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)

"""# Soft Attention Architecture Classes"""

BATCH_SIZE = 64
BUFFER_SIZE = 1000
embedding_dim = 256
units = 512
vocab_size = top_k + 1
num_steps = len(img_name_train) // BATCH_SIZE
features_shape = 2048
attention_features_shape = 49

class BahdanauAttention(tf.keras.Model): #Soft Attention
  def __init__(self, units):
    super(BahdanauAttention, self).__init__()
    self.W1 = tf.keras.layers.Dense(units)
    self.W2 = tf.keras.layers.Dense(units)
    self.V = tf.keras.layers.Dense(1)

  def call(self, features, hidden):
    
    hidden_with_time_axis = tf.expand_dims(hidden, 1)

    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +
                                         self.W2(hidden_with_time_axis)))

    score = self.V(attention_hidden_layer)

    attention_weights = tf.nn.softmax(score, axis=1)

    context_vector = attention_weights * features
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights

class CNN_Encoder(tf.keras.Model):

    def __init__(self, embedding_dim):
        super(CNN_Encoder, self).__init__()
        self.fc = tf.keras.layers.Dense(embedding_dim)

    def call(self, x):
        x = self.fc(x)
        x = tf.nn.relu(x)
        return x

class RNN_Decoder(tf.keras.Model):
  def __init__(self, embedding_dim, units, vocab_size):
    super(RNN_Decoder, self).__init__()
    self.units = units

    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')
    self.fc1 = tf.keras.layers.Dense(self.units)
    self.fc2 = tf.keras.layers.Dense(vocab_size)

    self.attention = BahdanauAttention(self.units)

  def call(self, x, features, hidden):

    context_vector, attention_weights = self.attention(features, hidden)

    x = self.embedding(x)
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

    output, state = self.gru(x)
    x = self.fc1(output)
    x = tf.reshape(x, (-1, x.shape[2]))
    x = self.fc2(x)

    return x, state, attention_weights

  def reset_state(self, batch_size):
    return tf.zeros((batch_size, self.units))

encoder = CNN_Encoder(embedding_dim)
decoder = RNN_Decoder(embedding_dim, units, vocab_size)


optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')


def loss_function(real, pred):
  mask = tf.math.logical_not(tf.math.equal(real, 0))
  loss_ = loss_object(real, pred)
  mask = tf.cast(mask, dtype=loss_.dtype)
  loss_ *= mask
  return tf.reduce_mean(loss_)

@tf.function
def train_step(img_tensor, target):
  loss = 0

  hidden = decoder.reset_state(batch_size=target.shape[0])

  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)

  with tf.GradientTape() as tape:
      features = encoder(img_tensor)

      for i in range(1, target.shape[1]):
          predictions, hidden, _ = decoder(dec_input, features, hidden)

          loss += loss_function(target[:, i], predictions)
          dec_input = tf.expand_dims(target[:, i], 1)

  total_loss = (loss / int(target.shape[1]))

  trainable_variables = encoder.trainable_variables + decoder.trainable_variables

  gradients = tape.gradient(loss, trainable_variables)

  optimizer.apply_gradients(zip(gradients, trainable_variables))

  return loss, total_loss

# listDataset = list(dataset)

# joblib.dump(listDataset,'/content/drive/MyDrive/DL/Assignment_4/dataset')

# dataset = joblib.load('/content/drive/MyDrive/DL/Assignment_4/dataset')

# len(list(dataset))

def train_data(dataset, EPOCHS=10):
  loss_plot = []
  
  for epoch in range(EPOCHS):
      start_time = time.time()
      total_loss = 0
      
      for (batch, (img_tensor, target)) in enumerate(dataset):
          
          batch_loss, t_loss = train_step(img_tensor, target)
          total_loss += t_loss
          
          if batch % 100 == 0:
              average_batch_loss = batch_loss.numpy()/int(target.shape[1])
              # print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')
              
      loss_plot.append(total_loss / num_steps)
      end_time = time.time()
      print('Epoch {0:d}/{1:d}'.format(epoch+1, EPOCHS), ": {0:.3f}sec".format((end_time - start_time)))
      print('===============>  train-loss=%.3f' % (total_loss/num_steps))
      
  return loss_plot

def plotting_loss(loss_plot):
  plt.plot(loss_plot)
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.title('Loss Plot')
  plt.show()

"""# Training Model"""

loss_plot = train_data(dataset, 10)

plotting_loss(loss_plot)

def convert_To_numPy(loss_plot):
  for i in range(len(loss_plot)):
    loss_plot[i] = loss_plot[i].numpy()
  return loss_plot

loss_plot__1 = convert_To_numPy(loss_plot)

loss_plot1 = train_data(dataset, 40)

loss_plot__2 = convert_To_numPy(loss_plot1)

loss_plot_3 = loss_plot__1 + loss_plot__2

plotting_loss(loss_plot_3)

loss_plot = train_data(dataset, 50)
plotting_loss(loss_plot)

image_path = image_names[6100]
result, attention_plot = evaluate(image_path, encoder, decoder)
print('Prediction Caption:', ' '.join(result))
plot_attention(image_path, result, attention_plot)
Image.open(image_path)

"""## Caption with Attention Weights, other helper functions"""

def evaluate(image, encoder, decoder):
    attention_plot = np.zeros((max_length, attention_features_shape))
    hidden = decoder.reset_state(batch_size=1)

    temp_input = tf.expand_dims(load_image(image)[0], 0)
    img_tensor_val = image_features_extract_model(temp_input)
    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],
                                                 -1,
                                                 img_tensor_val.shape[3]))

    features = encoder(img_tensor_val)

    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)
    result = []

    for i in range(max_length):
        predictions, hidden, attention_weights = decoder(dec_input)

        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()

        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()
        result.append(tokenizer.index_word[predicted_id])

        if tokenizer.index_word[predicted_id] == '<end>':
            return result, attention_plot

        dec_input = tf.expand_dims([predicted_id], 0)

    attention_plot = attention_plot[:len(result), :]
    return result, attention_plot

def plot_attention(image, result, attention_plot):
    temp_image = np.array(Image.open(image))
    fig = plt.figure(figsize=(20, 20))
    len_result = len(result)
    for i in range(len_result):
        temp_att = np.resize(attention_plot[i], (8, 8))
        grid_size = max(np.ceil(len_result/2), 2)
        ax = fig.add_subplot(grid_size, grid_size, i+1)
        ax.set_title(result[i])
        img = ax.imshow(temp_image)
        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())
        plt.axis('off')
    plt.tight_layout()
    plt.show()

rid = np.random.randint(0, len(img_name_val))
image = img_name_val[rid]
real_caption = ' '.join([tokenizer.index_word[i]
                        for i in cap_val[rid] if i not in [0]])
result, attention_plot = evaluate(image, encoder, decoder)
print('Real Caption:', real_caption)
print('Prediction Caption:', ' '.join(result))
plot_attention(image, result, attention_plot)

"""## *I am not able to save decoder so you have to train models and then check the results*"""

# encoder.save('/content/drive/MyDrive/DL/Assignment_4/Final_Encoder_Q1', save_format='tf')

# decoder.save_weights('/content/drive/MyDrive/DL/Assignment_4/Final_Decoder_Q1', save_format='tf')

# encd1 = load_model('/content/drive/MyDrive/DL/Assignment_4/Final_Encoder_Q1')

# embedding_dim = 256
# units = 512
# vocab_size = 5001
# optimizer = tf.keras.optimizers.Adam()
# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
# dcd1 = RNN_Decoder(embedding_dim, units, vocab_size)
# dcd1.compile(loss=loss_object, optimizer=optimizer)

# dcd1.load_weights('/content/drive/MyDrive/DL/Assignment_4/Final_Decoder_Q1')

"""## Try on images

"""

image_path = image_names[6819]
result, attention_plot = evaluate(image_path, encoder, decoder)
print('Prediction Caption:', ' '.join(result))
plot_attention(image_path, result, attention_plot)
Image.open(image_path)

image_path = image_names[6600]
result, attention_plot = evaluate(image_path, encoder, decoder)
print('Prediction Caption:', ' '.join(result))
plot_attention(image_path, result, attention_plot)
Image.open(image_path)

image_path = image_names[6545]
result, attention_plot = evaluate(image_path, encoder, decoder)
print('Prediction Caption:', ' '.join(result))
plot_attention(image_path, result, attention_plot)
Image.open(image_path)

image_path = image_names[7001]
result, attention_plot = evaluate(image_path, encoder, decoder)
print('Prediction Caption:', ' '.join(result))
plot_attention(image_path, result, attention_plot)
Image.open(image_path)

image_path = image_names[7789]
result, attention_plot = evaluate(image_path, encoder, decoder)
print('Prediction Caption:', ' '.join(result))
plot_attention(image_path, result, attention_plot)
Image.open(image_path)

image_path = '/content/drive/MyDrive/DL/Assignment_4/Data/Test/Images/1119015538_e8e796281e.jpg'
result, attention_plot = evaluate(image_path, encoder, decoder)
print('Prediction Caption:', ' '.join(result))
plot_attention(image_path, result, attention_plot)
Image.open(image_path)

"""# BLEU1-4 and METEOR

## BLEU 1-4
"""

def BLEU(image_names, captions, encoder, decoder, range1=6000 ,range2=7000, Bleu_wt = (1, 0, 0, 0)):
  scoreList = []
  for i in range(range1, range2):
    image_path = image_names[i]
    cap = captions[image_path]
    result, attention_plot = evaluate(image_path, encoder, decoder)
    result = result[:-1]
    max_score = -1
    for j in cap:
      score = sentence_bleu(j, result , weights=Bleu_wt)
      if score > max_score:
        max_score = score
    scoreList.append(max_score)
  return (sum(scoreList)*100)/len(scoreList)

"""## Individual BLEU Scores"""

BLEU_1 = BLEU(image_names, captions, encoder, decoder)
print('BLEU-1 for the Validation Data is {0:.3f}'.format(BLEU_1))

BLEU_1 = BLEU(image_names, captions, encoder, decoder, range1=7000 ,range2=8000)
print('BLEU-1 for the Test Data is {0:.3f}'.format(BLEU_1))

BLEU_2 = BLEU(image_names, captions, encoder, decoder, Bleu_wt = (0, 1, 0, 0))
print('BLEU-2 for the Validation Data is {0:.3f}'.format(BLEU_2))

BLEU_2 = BLEU(image_names, captions, encoder, decoder, range1=7000 ,range2=8000, Bleu_wt = (0, 1, 0, 0))
print('BLEU-2 for the Test Data is {0:.3f}'.format(BLEU_2))

BLEU_3 = BLEU(image_names, captions, encoder, decoder, Bleu_wt = (0, 0, 1, 0))
print('BLEU-3 for the Validation Data is {0:.3f}'.format(BLEU_3))

BLEU_3 = BLEU(image_names, captions, encoder, decoder, range1=7000 ,range2=8000, Bleu_wt = (0, 0, 1, 0))
print('BLEU-3 for the Test Data is {0:.3f}'.format(BLEU_3))

BLEU_4 = BLEU(image_names, captions, encoder, decoder, Bleu_wt = (0, 0, 0, 1))
print('BLEU-4 for the Validation Data is {0:.3f}'.format(BLEU_4))

BLEU_4 = BLEU(image_names, captions, encoder, decoder, range1=7000 ,range2=8000, Bleu_wt = (0, 0, 0, 1))
print('BLEU-4 for the Test Data is {0:.3f}'.format(BLEU_4))

"""## Cummulative"""

BLEU_2 = BLEU(image_names, captions, encoder, decoder, Bleu_wt = (0.5, 0.5, 0, 0)) # Cummulative
print('BLEU-2 for the Validation Data is {0:.3f}'.format(BLEU_2))

BLEU_2 = BLEU(image_names, captions, encoder, decoder, range1=7000 ,range2=8000, Bleu_wt = (0.5, 0.5, 0, 0))
print('BLEU-2 for the Test Data is {0:.3f}'.format(BLEU_2))

BLEU_2 = BLEU(image_names, captions, encoder, decoder, Bleu_wt = (0.33, 0.33, 0.33, 0))
print('BLEU 1-3 cummulative, for the Validation Data is {0:.3f}'.format(BLEU_2))

BLEU_2 = BLEU(image_names, captions, encoder, decoder, range1=7000 ,range2=8000, Bleu_wt = (0.33, 0.33, 0.33, 0))
print('BLEU 1-3 cummulative, for the Test Data is {0:.3f}'.format(BLEU_2))

BLEU_2 = BLEU(image_names, captions, encoder, decoder, Bleu_wt = (0.25, 0.25, 0.25, 0.25))
print('BLEU 1-4 cummulative, for the Validation Data is {0:.3f}'.format(BLEU_2))

BLEU_2 = BLEU(image_names, captions, encoder, decoder, range1=7000 ,range2=8000, Bleu_wt = (0.25, 0.25, 0.25, 0.25))
print('BLEU 1-4 cummulative, for the Test Data is {0:.3f}'.format(BLEU_2))

"""## METEOR"""

def METEOR(image_names, captions, encoder, decoder, range1=6000, range2=7000):
  scoreList4 = []
  for i in range(range1, range2):
    image_path = image_names[i]
    cap = captions[image_path][0].split()
    cap1 = captions[image_path][1].split()
    cap2 = captions[image_path][2].split()
    cap3 = captions[image_path][3].split()
    cap4 = captions[image_path][4].split()
    result, attention_plot = evaluate(image_path, encoder, decoder)
    result = result[:-1]
    s=""
    for m in result:
      s +=m + ' '
    c=""
    for m in cap:
      c +=m + ' '
    c1=""
    for m in cap1:
      c1 +=m + ' '
    c2=""
    for m in cap2:
      c2 +=m + ' '
    c3=""
    for m in cap3:
      c3 +=m + ' '
    c4=""
    for m in cap4:
      c4 +=m + ' '
    # print(s,c)
    try:
      score = meteor_score(s, c)
      score1 = meteor_score(s, c1)
      score2 = meteor_score(s, c2)
      score3= meteor_score(s, c3)
      score4 = meteor_score(s, c4)
    
      score= max(score,score1,score2,score3,score4)
      scoreList4.append(score)
    except:
      pass
  return (sum(scoreList4)*100)/len(scoreList4)

meteor1 = METEOR(image_names, captions, encoder, decoder)
print('METEOR for the Validation Data is {0:.3f}'.format(meteor1))

meteor2 = METEOR(image_names, captions, encoder, decoder, range1=7000, range2=8000)
print('METEOR for the Test Data is {0:.3f}'.format(meteor2))

meteor3 = METEOR(image_names, captions, encd1, dcd1) # After model loading
print('METEOR for the Validation Data is {0:.3f}'.format(meteor3))

meteor3 = METEOR(image_names, captions, encd1, dcd1) # After model loading (day later)
print('METEOR for the Validation Data is {0:.3f}'.format(meteor3))